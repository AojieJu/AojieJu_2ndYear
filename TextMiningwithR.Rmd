---
title: "Text Mining with R"
author: "Aojie Ju"
date: "11/18/2021"
output: pdf_document
---

```{r}
#Create a character vector.
text<-c("Because I could not stop for Death-",
"He kindly stopped for me-",
"The Carriage held but just Ourselves-",
"and Immortality")

text

#Turn it into a tidy text dataset.
library(dplyr) 
text_df<-tibble(line=1:4,text=text) #tibble builds a data frame here.

text_df

```

Keep in mind that a tibble is not compatible with tidy text analysis, since each row is made up of multiple combined words. So we need to convert this as a "one-token-per-document-per-row."

```{r}
#break the text into individual tokens (tokenization) and transform it to a tidy data structure.
library(tidytext)
text_df %>%
  unnest_tokens(word,text)
#Notice that unnest_tokens leaves out other columns, punctuations, and converts the tokens to lowercase. 
```
Then let's move on and do some additional tidying work. The "janeaustenr" package contains six novels of Jane Austen. The texts in a one-row-per-line format. We'll use "mutate()" to create columns linenumber and chapter.

```{r}
#Construct the dataframe in one-row-per line format.
library(janeaustenr)
library(dplyr)
library(stringr)

original_books<-austen_books()%>%
  group_by(book)%>%
  mutate(linenumber=row_number(),
         chapter=cumsum(str_detect(text,
                              regex("^chapter [\\divxlc]",
                                         ignore_case=TRUE)))) %>%
  ungroup()

original_books

#Restructure it in one-token-per-row format.
library(tidytext)
tidy_books<-original_books%>%
  unnest_tokens(word,text)

tidy_books
```

In many cases, we remove stop words. In package "tidytext," we have a dataset "stop_words" with an "anti_join()."

```{r}
data(stop_words)

tidy_books<-tidy_books%>%
  anti_join(stop_words)
```

How can we count the common words in all of the book? We use "count()" in package "dplyr."

```{r}
tidy_books%>%
  count(word, sort=TRUE)
```

